{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations and the setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "K - \\text{number of bandits that we play}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu_{i}, i \\in \\{0,\\dots,K-1\\} - \\text{distribution of the reward coming from the bandit } i\n",
    "$$\n",
    "\n",
    "$$\n",
    "(X_{a,t}) - \\text{ array of independent rewards}, X_{a,t} \\text{coming from the bandit} a \\text{ at time } t\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{k,t} - \\text{number of times we selected bandit } k \\text{ before time moment } t \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{k,t} := \\begin{cases}  \n",
    "\\infty, \\text{ if the state } k \\text{ was not visited yet;} \\\\\n",
    "\\frac{1}{N_k}\\sum\\limits_{t=1}^{t}X_{I_{t},i}\\mathbb{I}_{I_{t} = k},\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_rewards(n,K,r_seed,mu_means):\n",
    "    \"\"\"\n",
    "    Function to initialize random rewards\n",
    "    \"\"\"\n",
    "    #generate another random rewards\n",
    "    np.random.seed(r_seed)\n",
    "    rand_rewards = np.random.randn(n,K)\n",
    "    rand_rewards += mu_means.reshape((1,K))\n",
    "    return rand_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pseudo_regret(selected_states,mu_means):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        selected_indices - np.array of length n, indices of selected elements\n",
    "        mu_means - np.array of length K, mean rewards of the bandits\n",
    "    \"\"\"\n",
    "    n = len(selected_states)\n",
    "    mu_star = np.max(mu_means)\n",
    "    return mu_star*np.arange(n) - np.cumsum(mu_means[selected_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we implement $\\varepsilon$-greedy strategy. With $\\varepsilon$-greedy strategy, next action $A_t$ with probability $1 - \\varepsilon$ is selected as\n",
    "$$\n",
    "A_t = \\arg\\max_{k = 1,\\dots,K} \\hat{\\mu}_{k,t} \n",
    "$$\n",
    "and with probability $\\varepsilon$ it is chisen from the uniform distribution over $\\{0,\\dots,K-1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of bandits\n",
    "K = 10\n",
    "#number of games to play\n",
    "n = 1000\n",
    "#generate bandit means\n",
    "np.random.seed(216)\n",
    "mu_means = np.random.randn(K)\n",
    "mu_star = np.max(mu_means)\n",
    "print(mu_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eps_greedy(n,K,r_seed,eps,mu_means):\n",
    "    \"\"\"\n",
    "    Function to play eps-greedy strategy in the stationary environment \n",
    "    \"\"\"\n",
    "    achieved_rewards = []\n",
    "    selected_states = []\n",
    "    np.random.seed(r_seed)\n",
    "    #initialize bandit rewards\n",
    "    rewards = init_random_rewards(n,K,r_seed,mu_means)\n",
    "    #initialize state estimates\n",
    "    state_estimates = 10*np.ones(K,dtype=float)\n",
    "    #initialize number of visits to states\n",
    "    n_visits = np.zeros(K,dtype = int)\n",
    "    for n_game in range(n):\n",
    "        #choose, if we play greedily or explore\n",
    "            #exploitation\n",
    "            #exploration\n",
    "        achieved_rewards.append(rewards[n_game,new_state])\n",
    "        selected_states.append(new_state)\n",
    "        #update current state estimator\n",
    "    return np.asarray(achieved_rewards),np.asarray(selected_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for different values of $\\varepsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traj = 1000\n",
    "start_seed = 1453\n",
    "eps = [0,0.01,0.1]\n",
    "#arrays to store rewards\n",
    "rewards_greedy = np.zeros((len(eps),n_traj,n),dtype = float)\n",
    "pseudo_regret_greedy = np.zeros((len(eps),n_traj,n),dtype = float)\n",
    "selected_states_greedy = np.zeros((len(eps),n_traj,n),dtype = int)\n",
    "#sample trajectories\n",
    "for i in range(n_traj):\n",
    "    for j in range(len(eps)):\n",
    "        rewards_greedy[j,i],selected_states_greedy[j,i] = Eps_greedy(n,K,start_seed+i,eps[j],mu_means)\n",
    "        pseudo_regret_greedy[j,i] = compute_pseudo_regret(selected_states_greedy[j,i],mu_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"Testing greedy strategies\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_greedy[0,:],axis=0),color = 'r',label = '$\\\\varepsilon$ = 0')\n",
    "ax1.plot(np.mean(rewards_greedy[1,:],axis=0),color = 'g',label = '$\\\\varepsilon$ = 0.01')\n",
    "ax1.plot(np.mean(rewards_greedy[2,:],axis=0),color = 'b',label = '$\\\\varepsilon$ = 0.1')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"Regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_greedy[0,:],axis=0),color = 'r',label = '$\\\\varepsilon$ = 0')\n",
    "ax2.plot(np.mean(pseudo_regret_greedy[1,:],axis=0),color = 'g',label = '$\\\\varepsilon$ = 0.01')\n",
    "ax2.plot(np.mean(pseudo_regret_greedy[2,:],axis=0),color = 'b',label = '$\\\\varepsilon$ = 0.1')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\alpha$-UCB strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Alpha_UCB(n,K,r_seed,alpha,mu_means):\n",
    "    \"\"\"\n",
    "    Function to play alpha-UCB strategy in the stationary environment \n",
    "    \"\"\"\n",
    "    achieved_rewards = []\n",
    "    selected_states = []\n",
    "    #initialize bandit rewards\n",
    "    rewards = init_random_rewards(n,K,r_seed,mu_means)\n",
    "    np.random.seed(r_seed)\n",
    "    #initialize state estimates\n",
    "    state_estimates = np.zeros(K,dtype=float)\n",
    "    #initialize number of visits to states\n",
    "    n_visits = np.zeros(K,dtype = int)\n",
    "    #first we explore states\n",
    "    for n_game in range(K):\n",
    "        new_state = n_game\n",
    "        achieved_rewards.append(rewards[n_game,new_state])\n",
    "        #update current state estimate\n",
    "        state_estimates[new_state] += rewards[n_game,new_state]\n",
    "        n_visits[new_state] += 1\n",
    "        selected_states.append(new_state)\n",
    "    #now we apply UCB\n",
    "    for n_game in range(K,n):\n",
    "        #compute UCB\n",
    "        #update current state estimate\n",
    "    return np.asarray(achieved_rewards),np.asarray(selected_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traj = 1000\n",
    "start_seed = 1453\n",
    "alphas = [1,2,4,16]\n",
    "#arrays to store rewards\n",
    "rewards_alpha_ucb = np.zeros((len(alphas),n_traj,n),dtype = float)\n",
    "pseudo_regret_alpha_ucb = np.zeros((len(alphas),n_traj,n),dtype = float)\n",
    "selected_states_alpha_ucb = np.zeros((len(alphas),n_traj,n),dtype = int)\n",
    "#sample trajectories\n",
    "for i in range(n_traj):\n",
    "    for j in range(len(alphas)):\n",
    "        rewards_alpha_ucb[j,i],selected_states_alpha_ucb[j,i] = Alpha_UCB(n,K,start_seed+i,alphas[j],mu_means)\n",
    "        pseudo_regret_alpha_ucb[j,i] = compute_pseudo_regret(selected_states_alpha_ucb[j,i],mu_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"Testing $\\\\alpha$-UCB strategies\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[0,:],axis=0),color = 'b',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[1,:],axis=0),color = 'g',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 2')\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[2,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 4')\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[3,:],axis=0),color = 'm',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 16')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"Regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[0,:],axis=0),color = 'b',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[1,:],axis=0),color = 'g',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 2')\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[2,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 4')\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[3,:],axis=0),color = 'm',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 16')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"UCB vs greedy: mean rewards\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[0,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax1.plot(np.mean(rewards_greedy[1,:],axis=0),color = 'b',label = '$\\\\varepsilon$ = 0.01')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"UCB vs greedy: regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[0,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax2.plot(np.mean(pseudo_regret_greedy[1,:],axis=0),color = 'b',label = '$\\\\varepsilon$ = 0.01')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOSS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider a slight modification of UCB, MOSS (Minimax Optimal Strategy in the Stochastic case). The time horyzont $n$ is supposed to be known. Value of choosing bandit $k$ at time $t$ is computed as \n",
    "\n",
    "$$\n",
    "Q_{k,t} = \\begin{cases}\n",
    "    \\infty, \\text{ if the bandit } k \\text{ was not visited yet} \\\\\n",
    "    \\hat{\\mu}_{k,n} + \\sqrt{\\frac{\\max(0,\\log(n/KN_{k,t-1})}{N_{k,t-1}}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The action $A_t$ is chosen as $A_t = \\arg\\max_{k = 1,\\dots,K}Q_{k,t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOSS(n,K,r_seed,mu_means):\n",
    "    \"\"\"\n",
    "    Function to play alpha-UCB strategy in the stationary environment \n",
    "    \"\"\"\n",
    "    achieved_rewards = []\n",
    "    selected_states = []\n",
    "    #initialize bandit rewards\n",
    "    rewards = init_random_rewards(n,K,r_seed,mu_means)\n",
    "    np.random.seed(r_seed)\n",
    "    #initialize state estimates\n",
    "    state_estimates = np.zeros(K,dtype=float)\n",
    "    #initialize number of visits to states\n",
    "    n_visits = np.zeros(K,dtype = int)\n",
    "    #first we explore states\n",
    "    for n_game in range(K):\n",
    "        new_state = n_game\n",
    "        achieved_rewards.append(rewards[n_game,new_state])\n",
    "        #update current state estimate\n",
    "        state_estimates[new_state] += rewards[n_game,new_state]\n",
    "        n_visits[new_state] += 1\n",
    "        selected_states.append(new_state)\n",
    "    #now we apply UCB\n",
    "    for n_game in range(K,n):\n",
    "        #compute MOSS\n",
    "        #update current state estimate\n",
    "    return np.asarray(achieved_rewards),np.asarray(selected_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays to store rewards\n",
    "rewards_moss = np.zeros((n_traj,n),dtype = float)\n",
    "pseudo_regret_moss = np.zeros((n_traj,n),dtype = float)\n",
    "selected_states_moss = np.zeros((n_traj,n),dtype = int)\n",
    "#sample trajectories\n",
    "for i in range(n_traj):\n",
    "    rewards_moss[i],selected_states_moss[i] = MOSS(n,K,start_seed+i,mu_means)\n",
    "    pseudo_regret_moss[i] = compute_pseudo_regret(selected_states_moss[i],mu_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"UCB vs MOSS: mean rewards\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_alpha_ucb[0,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax1.plot(np.mean(rewards_moss,axis=0),color = 'b',label = 'MOSS')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"UCB vs MOSS: regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_alpha_ucb[0,:],axis=0),color = 'r',label = '$\\\\alpha$-UCB straregy, $\\\\alpha$ = 1')\n",
    "ax2.plot(np.mean(pseudo_regret_moss,axis=0),color = 'b',label = 'MOSS')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider learning an approach based on the numerical preferences $H_t(k)$, assigned to each bandit $k$. The larger is the preference, the more often that action is taken, yet the preference does not have an interpretation in\n",
    "terms of reward.\n",
    "\n",
    "\n",
    "Formally, at time step $t$ we select the slot machine according to  the distribution\n",
    "$$\n",
    "\\pi_t(k) = \\frac{e^{H_t(k)}}{\\sum\\limits_{b=1}^{K}e^{H_t(b)}}\n",
    "$$\n",
    "Initially we start with $H_1(k) = 0$, for any $k = 1,\\dots,K$. We aim to implement the stochastic gradient ascent with respect to the preferences $H_t(k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim at maximizing the expected reward \n",
    "$$\n",
    "\\mathbb{E}_{\\pi_t}[r_t] := \\sum\\limits_{k=1}^{K}\\pi_t(k)\\mu_{k} \\rightarrow \\max_{H_{t}(k)}.\n",
    "$$\n",
    "One potential solution is given by the gradient ascent\n",
    "$$\n",
    "H_{t+1}(k) = H_{t}(k) + \\eta\\frac{\\partial \\mathbb{E}_{\\pi_t}[r_t]}{\\partial H_{t}(k)},\n",
    "$$\n",
    "where $\\eta > 0$ is a step size. Unfortunately, we can not evaluate $\\mathbb{E}_{\\pi_t}[r_t]$ directly, but we can re-write $\\frac{\\partial \\mathbb{E}_{\\pi_t}[r_t]}{\\partial H_{t}(k)}$ as an expectation of something, that we can sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Recall that $\\frac{\\partial \\pi_t(i)}{\\partial H_t(j)} = \\pi_{t}(i)\\bigl(\\delta_{i,j} - \\pi_t(j)\\bigr)$. Show that \n",
    "$$\n",
    "\\frac{\\partial \\mathbb{E}_{\\pi_t}[r_t]}{\\partial H_{t}(k)} = \\mathbb{E}_{A_t \\sim \\pi_t}\\bigl[r_t(\\delta_{k,A_t} - \\pi_t(k))\\bigr]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_policy(H):\n",
    "    \"\"\"\n",
    "    input: preferences vector H_t\n",
    "    output: softmax policy pi_t, based on preferences H_t\n",
    "    \"\"\"\n",
    "    return np.exp(H)/np.sum(np.exp(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_bandit_v0(n,K,r_seed,eta,mu_means):\n",
    "    \"\"\"\n",
    "    Function to play alpha-UCB strategy in the stationary environment \n",
    "    input:\n",
    "        eta - step size of the gradient ascent;\n",
    "    output: rewards, indices of the selected elements\n",
    "    \"\"\"\n",
    "    achieved_rewards = []\n",
    "    selected_states = []\n",
    "    #initialize bandit rewards\n",
    "    rewards = init_random_rewards(n,K,r_seed,mu_means)\n",
    "    np.random.seed(r_seed)\n",
    "    #initialize preferences\n",
    "    H = np.zeros(K,dtype=float)\n",
    "    #main loop\n",
    "    for n_game in range(n):\n",
    "        #compute pi_t\n",
    "        pi = \n",
    "        #sample according to pi\n",
    "        new_state = np.random.choice(np.arange(K),size=1, p = pi)\n",
    "        new_state = new_state[0]\n",
    "        r_t = rewards[n_game,new_state]\n",
    "        achieved_rewards.append(r_t)\n",
    "        selected_states.append(new_state)\n",
    "        #do gradient step\n",
    "    return np.asarray(achieved_rewards),np.asarray(selected_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traj = 1000\n",
    "start_seed = 1453\n",
    "etas = [0.1,0.2]\n",
    "#arrays to store rewards\n",
    "rewards_gradient_bandit = np.zeros((len(etas),n_traj,n),dtype = float)\n",
    "pseudo_regret_grad = np.zeros((len(etas),n_traj,n),dtype = float)\n",
    "selected_states = np.zeros((len(etas),n_traj,n),dtype = int)\n",
    "#sample trajectories\n",
    "for i in range(n_traj):\n",
    "    for j in range(len(etas)):\n",
    "        rewards_gradient_bandit[j,i],selected_states[j,i] = gradient_bandit_v0(n,K,start_seed+i,etas[j],mu_means)\n",
    "        pseudo_regret_grad[j,i] = compute_pseudo_regret(selected_states[j,i],mu_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"Testing gradient strategies\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_gradient_bandit[0,:],axis=0),color = 'r',label = 'Gradient bandits, $\\\\eta$ = 0.1')\n",
    "ax1.plot(np.mean(rewards_gradient_bandit[1,:],axis=0),color = 'g',label = 'Gradient bandits, $\\\\eta$ = 0.2')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"Regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_grad[0,:],axis=0),color = 'r',label = 'Gradient bandits, $\\\\eta$ = 0.1')\n",
    "ax2.plot(np.mean(pseudo_regret_grad[1,:],axis=0),color = 'g',label = 'Gradient bandits, $\\\\eta$ = 0.2')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done to improve performance of the gradient-based policies? We can add a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Show that for any random variable $Y$, not depending on the action choice $I_t$, it holds \n",
    "$$\n",
    "\\frac{\\partial \\mathbb{E}_{\\pi_t}[r_t]}{\\partial H_{t}(k)} = \\mathbb{E}_{I_t \\sim \\pi_t}\\left[\\bigl(r_t - Y\\bigr)(\\delta_{k,I_t} - \\pi_t(k))\\right]\n",
    "$$\n",
    "A common way to select $Y$ is to use average reward until time $t$, that is,\n",
    "$$\n",
    "\\overline{r}_t = \\frac{1}{t}\\sum\\limits_{i=1}^{t}r_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_bandit_baseline(n,K,r_seed,eta,mu_means):\n",
    "    \"\"\"\n",
    "    Function to play alpha-UCB strategy in the stationary environment \n",
    "    input:\n",
    "        eta - step size of the gradient ascent;\n",
    "    output: rewards, indices of the selected elements\n",
    "    \"\"\"\n",
    "    achieved_rewards = []\n",
    "    selected_states = []\n",
    "    #initialize bandit rewards\n",
    "    rewards = init_random_rewards(n,K,r_seed,mu_means)\n",
    "    np.random.seed(r_seed)\n",
    "    #initialize preferences\n",
    "    H = np.zeros(K,dtype=float)\n",
    "    #baseline\n",
    "    r_bar = 0.0\n",
    "    #main loop\n",
    "    for n_game in range(n):\n",
    "        #compute pi_t\n",
    "        pi = \n",
    "        #sample according to pi\n",
    "        new_state = np.random.choice(np.arange(K),size=1, p = pi)\n",
    "        new_state = new_state[0]\n",
    "        r_t = rewards[n_game,new_state]\n",
    "        achieved_rewards.append(r_t)\n",
    "        selected_states.append(new_state)\n",
    "        #do gradient step\n",
    "        #update baseline\n",
    "    return np.asarray(achieved_rewards),np.asarray(selected_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays to store rewards\n",
    "rewards_gradient_baseline = np.zeros((len(etas),n_traj,n),dtype = float)\n",
    "pseudo_regret_grad_baseline = np.zeros((len(etas),n_traj,n),dtype = float)\n",
    "selected_states_baseline = np.zeros((len(etas),n_traj,n),dtype = int)\n",
    "#sample trajectories\n",
    "for i in range(n_traj):\n",
    "    for j in range(len(etas)):\n",
    "        rewards_gradient_baseline[j,i],selected_states_baseline[j,i] = gradient_bandit_baseline(n,K,start_seed+i,etas[j],mu_means)\n",
    "        pseudo_regret_grad_baseline[j,i] = compute_pseudo_regret(selected_states_baseline[j,i],mu_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"Testing gradient strategies\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_gradient_baseline[0,:],axis=0),color = 'b',label = 'Gradient bandits, $\\\\eta$ = 0.1')\n",
    "ax1.plot(np.mean(rewards_gradient_baseline[1,:],axis=0),color = 'r',label = 'Gradient bandits, $\\\\eta$ = 0.2')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"Regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_grad_baseline[0,:],axis=0),color = 'b',label = 'Gradient bandits, $\\\\eta$ = 0.1')\n",
    "ax2.plot(np.mean(pseudo_regret_grad_baseline[1,:],axis=0),color = 'r',label = 'Gradient bandits, $\\\\eta$ = 0.2')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8)) \n",
    "plt.title(\"Regret bounds\",fontsize=20)\n",
    "plt.plot(np.mean(pseudo_regret_grad[0,:],axis=0),color = 'b',label = 'Gradient bandits, $\\\\eta$ = 0.1')\n",
    "plt.plot(np.mean(pseudo_regret_grad_baseline[0,:],axis=0),color = 'r',label = 'Gradient bandit with baseline, $\\\\eta$ = 0.2')\n",
    "plt.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems that our baseline has absolutely no affect on the performance. Let us perturb the problem a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate bandit means\n",
    "np.random.seed(216)\n",
    "mu_means_new = 5 + np.random.randn(K)\n",
    "mu_star_new = np.max(mu_means_new)\n",
    "print(mu_star_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "#arrays to store rewards\n",
    "rewards_gradient_baseline = np.zeros((n_traj,n),dtype = float)\n",
    "pseudo_regret_grad_baseline = np.zeros((n_traj,n),dtype = float)\n",
    "selected_states_baseline = np.zeros((n_traj,n),dtype = int)\n",
    "#arrays to store rewards\n",
    "rewards_gradient_bandit = np.zeros((n_traj,n),dtype = float)\n",
    "pseudo_regret_grad = np.zeros((n_traj,n),dtype = float)\n",
    "selected_states = np.zeros((n_traj,n),dtype = int)\n",
    "\n",
    "for i in range(n_traj):\n",
    "    #play without baseline\n",
    "    rewards_gradient_bandit[i],selected_states[i] = gradient_bandit_v0(n,K,start_seed+i,eta,mu_means_new)\n",
    "    pseudo_regret_grad[i] = compute_pseudo_regret(selected_states[i],mu_means_new)\n",
    "    #play with baseline\n",
    "    rewards_gradient_baseline[i],selected_states_baseline[i] = gradient_bandit_baseline(n,K,start_seed+i,eta,mu_means_new)\n",
    "    pseudo_regret_grad_baseline[i] = compute_pseudo_regret(selected_states_baseline[i],mu_means_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "#first plot: average rewards\n",
    "ax1.set_title(\"Testing gradient strategies\",fontsize=20)\n",
    "ax1.plot(np.mean(rewards_gradient_bandit,axis=0),color = 'b',label = 'Gradient bandits without baseline, $\\\\eta$ = 0.1')\n",
    "ax1.plot(np.mean(rewards_gradient_baseline,axis=0),color = 'r',label = 'Gradient bandits with baseline, $\\\\eta$ = 0.1')\n",
    "ax1.legend(loc = 'lower right',fontsize = 20)\n",
    "\n",
    "#second plot: regret bounds\n",
    "ax2.set_title(\"Regret bounds\",fontsize=20)\n",
    "ax2.plot(np.mean(pseudo_regret_grad, axis=0),color = 'b',label = 'Gradient bandits without baseline, $\\\\eta$ = 0.1')\n",
    "ax2.plot(np.mean(pseudo_regret_grad_baseline, axis=0),color = 'r',label = 'Gradient bandits with baseline, $\\\\eta$ = 0.1')\n",
    "ax2.legend(loc = 'lower right',fontsize = 16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
